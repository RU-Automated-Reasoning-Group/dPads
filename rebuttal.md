# Reviewer 1

We appreciate your effort in providing detailed and helpful reviews. We address the concerns and questions as follows:

## Q1: Currently all the tasks are sequence classification tasks.

An important motivation of our work is to synthesize interpretable programs. We focused on behavior analysis applications where interpretability is an important concern. Our datasets are representative of real behavior data used by real domain experts (e.g. sports analysts for basketball). We will add a sequence regression task to the final version of our paper.

## Q2: No comparison with any enumerative approaches.

We have compared dPads with an enumeration strategy that synthesizes and evaluates complete programs in order of increasing complexity. This strategy is widely used in program synthesis tasks. We set the running time of the enumeration strategy twice as long as dPads's synthesis time.

|                  | Crim13 |      | Fly-vs-fly |  | Bball | | Sk152 |   |
|:----------------|:------:|:----:|:-------------:|:----------:|:----:|:------:|:-----:|:----:|
|                  | **F1**     | **Acc.** | **F1** | **Acc.** | **F1**    | **Acc.** | **F1**    | **Acc.** |
| **Enumeration** | .294 | .856 | .850 | .774 | .795  | .767 | .288  | .284 |
| **dPads**            | .458   | .812 | .887 | .853 | .945  | .939 | .337  | .337 |

dPads outperforms the enumeration strategy on all of the four benchmarks. Although the enumeration strategy gets higher accuracy on Crim13, it underfits this unbalanced dataset as the F1 score is very low. We have also tried a Monte-Carlo tree search strategy. Surprisingly, its performance is even worse than the simple enumeration strategy. We will provide additional details in the final version.

## Q3: No ablation study on dPads without node sharing and/or iterative unfolding.

We started our research without considering node sharing and iterative graph unfolding but found that dPads does not work well without these two optimization strategies. The reason is that the size of a program derivation graph grows exponentially large with the height of its search tree (e.g. Figure 2) and the number of DSL functions (e.g. Fold and ITE). Without the two optimizations, limited by the size of GPU memory, dPads may either time-out or encounter out-of-memory error when searching programs that need deep structures to ensure high accuracy. We report the comparison results over 5 random runs as follows. Costs of time are set in minutes.

|                  | Crim13 |      |        | Fly-vs-fly |      |        | Bball |      |        | Sk152 |      |        |
|:----------------|:------:|:----:|:------:|:----------:|:----:|:------:|:-----:|:----:|:------:|:-----:|:----:|--------|
|                  | **F1**     | **Acc.** | **Time**   | **F1**         | **Acc.** | **Time**   | **F1**    | **Acc.** | **Time**   | **F1**    | **Acc.** | **Time**   |
| **dPads w/o node sharing**  | .453   | .800 | 334.93 | -          | -    | >24hrs | -     | -    | >24hrs | .321  | .322 | 252.81 |
| **dPads w/o graph unfolding** | .449   | .818 | 280.67 | -          | -    | OOM    | .848  | .832 | 348.09 | .348  | .346 | 273.95 |
| **dPads**            | .458   | .812 | 147.87 | .887       | .853 | 348.25 | .945  | .939 | 174.68 | .337  | .337 | 162.70 |


The results demonstrate that dPads with the two optimization strategies converges much faster.

On Fly-vs-fly, without iterative graph unfolding, the program derivation graph generated by dPads has 33 nodes shared by 485 operations, 32 edges, and a total of 13824 parameters to train. dPads encounters an out-of-memory (OOM) exception when training this graph. In contrast, with iterative graph unfolding, the deepest graph has only 21 nodes shared by 220 operations, 20 edges, and a total of 5497 parameters to train. dPads converges in less than 360 mins.

On Basketball, without node sharing, the deepest program derivation graph generated by dPads has 94 nodes hosting 302 operations, 93 edges, and a total of 8637 parameters to train. The search procedure of dPads (Sec 3.3) does not terminate in 24 hours because of the many nodes and edges. In contrast, with node sharing, the program derivation graph reduces to only 29 nodes shared by 152 operations, 28 edges, and a total of 4157 parameters. dPads converges in less than 180 mins.

Moreover, training without these two optimizations does not necessarily produce better results even when there is not OOM or timeout. For example, on Basketball, dPads achieves .945 F1 score. dPads without iterative graph unfolding only obtains .848 F1 score. We suspect this is because the program derivation graph without iterative unfolding is more difficult to train as it contains significantly more parameters (6791 vs 4157). In contrast, dPads uses top-N preservation to efficiently prune away low-quality options. We will provide additional details in the final version.

## Q4: Lack of discussion on the limitations of dPads.

**Performance gap to RNN.** As the reviewer has pointed out, our paper acknowledged that dPads's performance does not match the RNN baseline. This is mainly due to the limitations in expressivity imposed by the DSL. Firstly, in dPads's DSL, we only allow for customized feature functions $F_{S,\theta}(x)$ that extract a vector consisting of a predefined subset $S$ of the dimensions of an input sequence $x$ and pass the extracted vector through a linear function with trainable parameters $\theta$. For example, for Crim13, we predefine feature functions such as the XY positions, angles, velocity, acceleration, distance of a pair of mice, and distance difference for every two consecutive frames. We list the details of $F_{S,\theta}(x)$ for each dataset in Table 3, 4, 5, 6 in Appendix C. These feature functions are extremely helpful to ensure that a synthesized program composed of these functions is interpretable. However, a program limited to these predefined feature functions may be suboptimal as only a subset of features is used. Instead, an RNN policy can understand the whole context of a sequence using all features available from the input. Secondly, for the sake of interpretation, our DSL also predefines a limited set of algebraic operations to process the outputs from the feature functions. However, an RNN can use a more expressive activation function to learn about long-term dependencies in data. We have reported the performance limitation of dPads in Table 1.

**Program Derivation Graph Accuracy.** Another important limitation is that performance estimation of each program included in a program derivation graph ranked by architecture weights can be inaccurate due to the co-adaption among operations (node sharing). As one super program derivation graph may not be able to model the entire search space accurately, we have used multiple sub program derivation graphs generated on the fly via a variant of A$^*$ search to address this limitation (Section 3.3). Each sub derivation graph models one part of the search space. However, as reported in the ablation study (Table 2 and Appendix A), the A$^*$ search slows down the whole synthesis procedure, despite improving the accuracy, and may need additional optimization.

## Q5: Does each node have at most N subterms after top-N preservation?

After top-N preservation, each node may contain more than N subterms. We preserve the top-N subterms in a node $n$ for each term in $n$'s parent. For example, consider node 3 of the program derivation graph in Figure 3. The parent of node 3 is node 1. Assume N=1. After top-1 preservation on node 3, it contains 2 subterms *x* and *Fold* according to the $w$ matrix in Figure 3. *x* is retained on node 3 for the *Add* function on the parent node 1 and *Fold* is retained for the *ITE* function on node 1. We will provide this example in the final version.

**Other Comments.** In addition to these main points, we want to thank the reviewer for the helpful suggestions on how to further improve the paper. We plan on incorporating all of these, such as a light introduction to the Program Derivation Graph including a simple program derivation tree and the corresponding graph, correcting the use of $\sigma$ for various purposes, swapping Figure 5(b) and Figure 5(c) to keep the same sequence as in Table 1. The detailed discussion on further clarifications is deeply appreciated as well. We will add comparison with the enumerative approaches to emphasize dPads's benefits (Q2), include the results without node sharing and/or iterative unfolding (Q3), and discuss the limitations of dPads in depth (Q4) in the final version.


# Reviewer 2

We appreciate your effort in providing detailed and helpful reviews. We address the concerns and questions as follows:

## Q1: dPads's results are far behind the RNN's to make the accuracy / interpretability trade-off significant.

We agree with the reviewer that RNN models in general have better performance than programmatic models learned by dPads. However, in our experiment, we found that on several domains the programmatic models have comparable performance with the RNN models.
* On the Crim13 domain, the programmatic model in Figure 6(a) achieves a high F1 score 0.475, only 0.006 less than the RNN model. Another synthesized programmatic model on this domain (line 720 in the appendix) achieves a comparable high F1 score 0.468.

* On the Fly-vs-fly domain, the learned programmatic model (line 729 in the appendix) achieves a high F1 score 0.904, only 0.06 less than the RNN model.

* On the Basketball domain, the learned programmatic model (line 747 in the appendix) also achieves a high F1 score 0.945, only 0.035 less than the RNN model.

In general, our results verify that our programmatic models on the above three domains are comparable with highly expressive RNNs. Observe that there is at most an 8\% drop in F1 scores when using the learned programmatic models. Thus, we believe that our results are sufficient to support our claim that dPads discovers programmatic classifiers that yield natural interpretations and achieve competitive quality.

More importantly, dPads's performance outperforms NEAR, the current state-of-the-art method for program learning from a single dataset. dPads opens up a new promising line of research to address the accuracy / interpretability trade-off in Machine Learning.

## Q2: The benchmarks are quite similar to one another and all come from visual recordings over time.

As the reviewer has pointed out that our approach is more general than this one modality. An important motivation of our work is to synthesize interpretable programs. We focused on behavior analysis applications where interpretability is an important concern. Our datasets are representative of real behavior data used by real domain experts (e.g. sports analysts for basketball, and neuroscientists for Fly-vs-Fly). We will add a sequence regression task to the final version of our paper.

## Q3: The variance in run time can be quite high.

The variance in run time is high because dPads may synthesize programs with different sizes due to randomness in stochastic gradient descent. For example, we show two programs synthesized by dPads as follows (also available in the appendix line 704, 720).  

**Program 1:**
```
Map(
    if AccelerationSelect$_{\theta_1}$($x_t$)
    then if PositionSelect$_{\theta_2}$($x_t$)
         then PositionSelect$_{\theta_3}$($x_t$)
         else VelocitySelect$_{\theta_4}$($x_t$)
    else Multiply(DistanceSelect$_{\theta_5}$($x_t$), DistanceSelect$_{\theta_6}$($x_t$))) $x$

```
**Program 2:**
```
MapPrefixes(
    Fold(
        if DistanceSelect$_{\theta_1}$($x_t$)
        then PositionSelect$_{\theta_2}$($x_t$)
        else PositionSelect$_{\theta_3}$($x_t$))) $x$
```
The two programs have similar F1 scores. As a result, during search, the two sub program derivation graphs led by "Map" and "MapPrefixes" also have very similar F1 scores. Due to the randomness in stochastic gradient descent, either one might be a little higher than the other and hence chosen firstly by the search algorithm. dPads spends more time on synthesizing Program 1 because it searches deeper in the program architecture space for this program as the program's abstract syntax tree is deeper. We will provide additional details in the final version.

## Q4: Empirical information about the reduction in search space provided by node sharing and top-N preservation.

We thank the reviewer for the great suggestion. We quantify the program derivation graph space reduction by node sharing and iterative unfolding as follows. We show for each benchmark the number of nodes (# Node), the total number of DSL functions hosted by the nodes (# Oper.), the total number of edges (# Edges), the number of architecture weights (# Weights), and the number of unknown program parameters (# Param.) to train on its program derivation graph. We only show the results of the deepest program derivation graph generated by dPads on each benchmark.

|                            | Crim13 |         |         |           |                     |  Sk152 |         |         |           |          | Fly-vs-fly |         |         |           |          | Basketball |         |         |           |          |
|----------------------------|:------:|---------|---------|-----------|---------------------|:------:|---------|---------|-----------|----------|:----------:|---------|---------|-----------|----------|:----------:|---------|---------|-----------|----------|
|                            | # Nodes | # Oper. | # Edges | # Weights | # Param.            | # Nodes | # Oper. | # Edges | # Weights | # Param. | # Nodes     | # Oper. | # Edges | # Weights | # Param. | # Nodes     | # Oper. | # Edges | # Weights | # Param. |
| dPads w/o graph unfolding            | 33     | 297     | 32      | 707       | 914                 | 33     | 337     | 32      | 681       | 48636    | 33         | 485     | 32      | 1021      | 12803    | 33         | 248     | 32      | 510       | 6281     |
| dPads w/o node sharing | 82     | 519     | 81      | 518       | 1814                | 60     | 196     | 59      | 195       | 34252     | 86         | 441     | 85      | 440       | 10612    | 94         | 302     | 93      | 301       | 8336     |
| dPads            | 15     | 100     | 14      | 203       | 315                 | 15     | 57      | 14      | 89        | 7298     | 21         | 220     | 20      | 370       | 5127     | 29         | 152     | 28      | 259       | 3898     |

Notice that dPads without iterative graph unfolding and dPads without node sharing generate program derivation graphs that are significantly larger.

For example, on Fly-vs-fly, without iterative graph unfolding, the program derivation graph has 33 nodes shared by 485 operations, 32 edges, and a total of 13824 parameters to train. Directly applying dPads to train under this setting encounters an out-of-memory (OOM) exception. In contrast, with iterative graph unfolding, the deepest graph generated by dPads has only 21 nodes shared by 220 operations, 20 edges, and a total of 5497 parameters to train. dPads converges in less than 360 mins.

On Basketball, without node sharing, the deepest program derivation graph generated by dPads has 94 nodes hosting 302 operations, 93 edges, and a total of 8637 parameters to train. If directly running dPads under this setting, the search procedure (Sec 3.3) does not terminate in 24 hours because of the many nodes and edges. In contrast, with node sharing, the program derivation graph reduces to only 29 nodes shared by 152 operations, 28 edges, and a total of 4157 parameters. dPads converges in less than 180 mins.

We will add the quantification results into the final version of the paper.


## Q5: F1 score and accuracy begin to drop off when N=3.

The result of our ablation study is duplicated as follows to ease communication. Our interpretation of the result is that dPads gets similar accuracy and F1 scores when setting N = 3 compared to N = 2 but consumes more time.

|        | N | dPads |      |        |         |           |
|:------:|---|-------|------|--------|---------|-----------|
|        |   | **F1**    | **Acc.** | **Time**   | **Std. F1** | **Std. Acc.** |
| **Crim13** | 1 | .272  | .627 | 50.85  | .111    | .218      |
|        | 2 | .458  | .812 | 147.87 | .014    | .008      |
|        | 3 | .450  | .811 | 441.12 | .025    | .008      |
| **Sk152**  | 1 | .310  | .310 | 40.34  | .020    | .024      |
|        | 2 | .337  | .337 | 162.70 | .017    | .017      |
|        | 3 | .336  | .338 | 609.14 | .011    | .010      |


We appreciate it if the reviewer could provide some clarification of the question in the rolling discussion phase as we do not observe that F1 score or accuracy begins to drop off when N = 3.

## Q6: Line 240: The description of $\mathcal{T}$ is not very clear

$\mathcal{T}$ is encoded exactly using Equation (3) in line 178 of the paper. We will provide additional details in the final version.

## Q7: Much of Appendix B appears in the paper and is duplicated.

In Appendix B, we have provided additional theoretical analysis on why dPads can optimally balance structure cost and program performance.

**Other Comments.** In addition to these main points, we want to thank the reviewer for the helpful suggestions on how to further improve the paper. We plan on incorporating all of these, such as explaining the high variance in run time (Q3) and including empirical information about the reduction in search space provided by node sharing and top-N preservation (Q4). We will also clarify the accuracy / interpretability trade-off (Q1).

# Reivewer 3

We appreciate your effort in providing detailed and helpful reviews. We address the concerns and questions as follows:

## Q1: How does dPads differ from NEAR? Why does dPads outperform NEAR?

Although dPads and NEAR both formalize program synthesis as a graph search problem, the two techniques are completely different. We compare dPads and NEAR as follows.

**NEAR**. Typically, search-based program synthesizers enumerate the underlying program space in some order and for each program checks whether or not it satisfies the synthesis constraints. It is a challenging problem because the architecture search space is combinatorial. The most simple strategy that starts by searching for programs with 1 DSL production rule and iteratively increases this bound does not scale to complex programs. NEAR uses neural models to approximate unexpanded subexpressions to estimate the likelihood of eventually deriving a high-quality program by choosing a particular production rule. It leverages this kind of information to prioritize promising search directions and hence can greatly accelerate the search process. However, NEAR's search strategy is still based on enumeration. When the search space is intractably large, enumeration-based strategies are inefficient in general.

**dPads's Contributions**. dPads proposes a new, scalable program synthesis technique. It views program architecture search as learning a probability distribution over *all* possible program architectures induced by a DSL. Unlike NEAR that enumerates and evaluates each program by training unknown parameters from scratch, dPads reduces the computation cost by training one program, a.k.a. program derivation graph, to approximate the performance of every program in the search space. It learns the architecture weights of each program encoded in a program derivation graph in a way that ranks the performance estimation of these programs. The search procedure in dPads is more efficient because it supports gradient-based architecture optimization in a novel continuous relaxation of the program architecture search space.

**dPads's Impacts.** To the best of our knowledge, dPads is the *first* program synthesis technique that applies gradient-based architecture search. Experiment results on sequence classification demonstrate that thanks to this strategy, dPads can efficiently search in a much deeper program space to learn more complex programs that are necessary to ensure higher F1 scores. NEAR often cannot reach the necessarily deep program space within the time budget because of its less efficient enumeration-based search procedure, resulting in suboptimal programs (see Q2 and Table 8 in the appendix).

## Q2: If dPads outperforms NEAR, why is NEAR faster than dPads on Fly-vs-fly, and similar in speed on Crim13 and Sk152?

In our experiments, compared to NEAR, dPads can more efficiently reach a much deeper program space (measured by abstract syntax tree heights). The results are as follows (also given in Table 8 of the appendix) where we show for each synthesized program its accuracy, F1 score, the averaged number of DSL production rules used to derive the program (#R), and the running time (#T). Costs of time are in minutes.

|             | Crim13 |      |         |         | Fly-vs-fly |      |         |         | Bball |      |        |         | SK152 |      |         |         |
|-------------|--------|------|---------|---------|------------|------|---------|---------|-------|------|---------|---------|-------|------|---------|---------|
|             | **F1**     | **Acc.** | **#R** | **#T** | **F1**         | **Acc.** | **#R** | **#T** | **F1**    | **Acc.** | **#R** | **#T** | **F1**    | **Acc.** | **#R** | **#T** |
| **A\*-NEAR**     | .286   | .820 | 6.8     | 164.92 | .828       | .764 | 2.8     | 243.82 | .940  | .934 | 8.0     | 553.01 | .312  | .315 | 4.2     | 210.23 |
| **IDS-BB-NEAR** | .323   | .834 | 5.8     | 463.36 | .822       | .750 | 2.4     | 465.57 | .793  | .768 | 7.0     | 513.33 | .314  | .317 | 4.4     | 848.44 |
| **dPads**       | .458   | .812 | 10.2    | 147.87 | .887       | .853 | 6.8     | 348.25  | .945  | .939 | 8.0     | 174.68 | .337  | .337 | 7.6     | 162.70 |

On Fly-vs-fly, although NEAR runs faster, it only finds programs derived by ~2.8 production rules but dPads finds much deeper programs derived by ~6.8 production rules. Consequently, for this benchmark, dPads achieves much higher accuracy and F1 scores. A similar trend can be observed for the results on Crim13 and Sk152. On Basketball, both tools find programs with ~8 production rules. In this case, the architecture spaces searched by the two tools are roughly equivalent, but dPads is 3 times faster.

The experiment results consistently demonstrate that dPads's gradient-based architecture search is much more efficient than NEAR's enumeration-based strategy. Moreover, NEAR uses neural models to estimate the performance of *a* partially expanded program. Our experiments find that due to overfitting or underfitting, such a neural model may be biased on *a* particular program. For example, on Fly-vs-fly, due to the biased estimation, NEAR stops searching the architecture space deeper than that contains programs derived by only ~2.8 production rules. In contrast, (1) dPads uses a sub program derivation graph to estimate the performance of a partially expanded program that can provide more accurate assessment due to the graph's syntax resemblance to a valid program; (2) dPads only uses neural models to provide "contrastive" performance estimation for a *set* of programs sharing nodes in a program derivation graph when iteratively unfolding the graph. As a result, on Fly-vs-fly, dPads searches the architecture space much deeper containing programs derived by ~6.8 production rules. Even dPads typically searches much deeper, it often runs faster than NEAR.

In the sequence classification tasks, although dPads learns more complex programs, synthesized programs' classification error plus architecture cost (i.e. the search objective) is still lower than that of the programs learned by NEAR. This demonstrates that dPads synthesizes more complex programs that are necessary to ensure higher F1 scores.


## Q3: Does NEAR already adopt any similar strategies? How novel are dPads's search strategies?

**NEAR does not attempt any search techniques proposed in dPads.** The NEAR authors acknowledge in their paper (page 9) that they only consider enumeration-based search because DARTS-style, gradient-based architecture search cannot be naturally applied to program synthesis due to the complexity of programming languages (e.g. the DSL in Figure 1). They gave two reasons to justify this claim. Firstly, different sets of operations take different input and output types and may only be available at different points of a program. Secondly, there is no fixed bound on the number of expressions in a program architecture.

**dPads's search strategy is novel** because it overcomes the two challenges raised in the NEAR paper. For the first time, dPads demonstrates that gradient-based architecture search is applicable to program synthesis and it substantially outperforms enumeration-based program learning methods including NEAR (reasons are explained in Q1). Node sharing and iterative graph unfolding are two optimizations that further scale dPads to complex programs. Observe that the size of a program derivation graph grows exponentially large with the height of its search tree and the number of functions in a DSL. If without the two optimizations, limited by the size of GPU memory, dPads cannot search programs with deep structures. These two techniques do not appear in NEAR as they are specific to dPads's gradient-based architecture search.

## Q4: Is dPads's A$^*$ Search any different than NEAR's search?

It appears that dPads and NEAR both use A$^*$ with admissible heuristics to perform a search within a program derivation graph. However, the purposes are quite different. dPads uses A$^*$ to overcome one potential disadvantage of differentiable architecture search: the performance estimation ranked by architecture weights in a program derivation graph can be inaccurate due to the co-adaption among operations (node sharing). While one super program derivation graph may not be able to model the entire search space accurately, dPads uses multiple sub program derivation graphs to effectively address the limitation by having each sub graph modeling one part of the search space. As a node in a program derivation graph contains multiple operations (e.g. Figure 3), dPads separates the entire search space into disjoint partitions by picking one operation from the compound node and assigns a sub program derivation graph to model each partition. This procedure is done in a recursive manner using A$^*$ search to prioritize partitions that include optimal programs.

Moreover, unlike NEAR, the A$^*$ search in dPads does not use neural models to estimate the performance of a partially expanded program. This is because our experiments find that due to overfitting or underfitting, a neural model may be biased on a particular program. In contrast, dPads uses a sub program derivation graph to estimate the performance of a partially expanded program. We find that it can provide more accurate assessment due to the graph's syntax resemblance to a valid program. We only use neural models to provide "contrastive" performance estimation for a *set* of programs sharing nodes in a program derivation graph for iterative graph unfolding (see Q2 for more explanation using the Fly-vs-fly benchmark). We will provide additional details in the final version.


## Q5: The paper needs a significant change to present the approach in a way that adequately explains its relationship to NEAR.

We thank the reviewer for the thoughtful comment. We disagree that the paper needs a major update to illustrate the relationship to NEAR. The two methods are substantially different. As explained in Q1 and Q2, NEAR enumerates the underlying program space in some order and for each program checks whether or not it satisfies the synthesis constraints by training unknown parameters from scratch. In contrast, dPads reduces the computation cost by training one program, a.k.a. program derivation graph, to approximate the quality (e.g. classification accuracy) of every program in the search space.

In the paper, Section 1 summarizes the aforementioned key differences, points out that enumeration-based strategies are inefficient in general since the search space can be intractably large, motives how and why gradient-based architecture search in dPads can come to rescue. In Section 3 (the main technical section), we focus on dPads's optimization algorithms that scale gradient-based architecture search to complex program synthesis problems. This section additionally describes how dPads uses graph partitioning (guided by A$^*$ search) to address the shortcoming of only using one program derivation graph to model the entire search space. Since these search techniques are specific to dPads, we do not need to discuss NEAR in the main body of the paper in depth.

To address the reviewer's concern, we will give a more detailed review of NEAR in Section 1 to help readers more deeply understand the challenges faced by enumeration-based program architecture search methods including NEAR. We will leverage this detailed review to further help readers obtain stronger intuitive understandings of how and why dPads's search strategies overcome the challenges. We hope this would meet the reviewer's expectations.

## Q6: Each dataset has a training, validation, and test set. Furthermore, the training set is split into $D_{val}$ and $D_{train}$. Where is each split used?

We give the full details of each split in line 589 of the appendix. In our experiments, we use the training datasets to optimize the architecture weights and unknown program parameters in a program derivation graph. During training, we additionally split the training set into $D_{val}$ and $D_{train}$ for optimizing architecture weights and unknown program parameters respectively (Equation 2). To avoid overfitting, when learning final programmatic classifiers during A$^*$ search, we use the validation datasets to obtain F1 scores to calculate the A$^*$ heuristic function $h$ (line 239 of the paper) on an optimized sub program derivation graph. We use the test datasets to obtain the final accuracy and F1 scores of synthesized programmatic classifiers.

## Q7: At line 307, why is the F1 score 0.475 of a program for Crim13 different from the value in Table 1, 0.458? Is it because Figure 6 contains the best program over 5 runs, and the other runs produce programs with much lower F1 score?

In Table 1, we report the F1 score 0.458 as the average of 5 random runs. In Figure 6, we give one program learned for Crim13 that has an F1 score 0.475. We show two other programs learned for Crim 13 on page 19 of the appendix. All the 5 programs synthesized by dPads have similar F1 scores 0.4676, 0.4752, 0.4558, 0.4427, 0.4471 as depicted in Figure 5(a). Figure 5 demonstrates that dPads consistently outperforms NEAR in achieving higher F1 scores and has significantly lower variance in F1 scores.

## Q8: Where do the costs $c(r)$ come from?

We explain in line 622 of the appendix our definition of the costs $c(r)$.  We set $c(r) = 1$ for any grammar rule $r$ and aim to learn a program that can be derived by as few grammar rules as possible. We report the average numbers of grammar rules used to construct learned programmatic classifiers in Table 8 of the appendix (See Q2 for more details).

## Q9: Have you extended NEAR to use the extra DSL elements, or limited dPads to use NEAR’s DSL?

For a fair comparison, we limit dPads to use exactly the same DSLs used to evaluate NEAR.

## Q10: Table 2: The F1, Acc, and Time columns are exactly the same between Crim13 and Sk152.

We apology for the confusion. We have corrected Table 2 in Appendix A.

**Other Comments.** In addition to these main points, we want to thank the reviewer for the helpful suggestions on how to further improve the paper. We plan on incorporating all of these, such as more thoroughly comparing dPads to NEAR early in the paper, making a connection between iterative graph unfolding and beam search, and pointing out that dPads requires more time and resources to produce a program compared to plain RNN methods.

# Reviewer 4

We appreciate your effort in providing detailed and helpful reviews. We address the concerns and questions as follows:

## Q1: Not a breakthrough in applying neural program search to complex problems.

Broadly speaking, dPads is different than neural program induction techniques, as the outputs of these approaches are neural networks as opposed to programs. dPads is also different than metalearning-based approaches such as [21, 34] because dPads must learn a program from a single dataset. We believe learning from a single dataset is an important research problem because in many applications we may not have a corpus of datasets and corresponding programs available.

State-of-the-art program learning methods on learning from a single dataset such as NEAR typically enumerate the underlying program space in some order and for each program checks whether or not it satisfies the synthesis constraints by training unknown program parameters from scratch. Since the search space can be intractably large, enumeration-based strategies are inefficient in general.

dPads is the *first* program synthesis algorithm that applies gradient-based search in a program architecture space. It efficiently reduces the computation cost of enumeration by training one program, a.k.a. a program derivation graph, to approximate the quality (e.g. classification accuracy) of every program in the search space. It learns the architecture weights of each program encoded in a program derivation graph in a way that ranks the performance estimation of these programs. Experiment results demonstrate that thanks to this strategy, dPads can efficiently search in a much deeper program space than enumeration-based methods such as NEAR to learn more complex programs that are necessary to ensure higher F1 scores. NEAR often cannot reach the necessarily deep program space within the time budget because of its less efficient enumeration-based search strategy and less effective neural heuristics (see Table 8 in the appendix for more details).


## Q2: Node-sharing not novel or similar to the TerpreT work of Gaunt et al.

We agree with the reviewer that node sharing itself is not novel. Other than TerpreT, many neural architecture search methods heavily use this idea to accelerate search. However, dPads's core contribution is the *first* program synthesis algorithm that applies gradient-based search in the discrete space of program architectures.

More specifically, existing differentiable programming techniques including TerpreT consider a parameterized representation of programs (essentially, an "architecture" in the language of our paper), and the learning objective is to find optimal parameters for this representation. In contrast, dPads searches over the nonparametric space of all architectures expressed in a rich programming language. The only methods we know that searches over architectures of differentiable programs are based on search space enumeration, e.g. NEAR. For the first time, dPads demonstrates that gradient-based architecture search is applicable to program synthesis and it substantially outperforms enumeration-based program learning methods. Node sharing and iterative graph unfolding are two optimizations that further scale dPads to complex programs.

## Q3: What would happen if every potential function would have its own child nodes? What is the effect of the iterative search tree deepening technique?

We started our research without considering node sharing and iterative graph unfolding but found that dPads does not work well without these two optimization strategies. The reason is that the size of a program derivation graph grows exponentially large with the height of its search tree (e.g. Figure 2) and the number of DSL functions (e.g. Fold and ITE). Without the two optimizations, limited by the size of GPU memory, dPads may either time-out or encounter out-of-memory error when searching programs that need deep structures to ensure high accuracy. We report the comparison results over 5 random runs as follows. Costs of time are set in minutes.

|                  | Crim13 |      |        | Fly-vs-fly |      |        | Bball |      |        | Sk152 |      |        |
|:----------------|:------:|:----:|:------:|:----------:|:----:|:------:|:-----:|:----:|:------:|:-----:|:----:|--------|
|                  | **F1**     | **Acc.** | **Time**   | **F1**         | **Acc.** | **Time**   | **F1**    | **Acc.** | **Time**   | **F1**    | **Acc.** | **Time**   |
| **dPads w/o node sharing**  | .453   | .800 | 334.93 | -          | -    | >24hrs | -     | -    | >24hrs | .321  | .322 | 252.81 |
| **dPads w/o graph unfolding** | .449   | .818 | 280.67 | -          | -    | OOM    | .848  | .832 | 348.09 | .348  | .346 | 273.95 |
| **dPads**            | .458   | .812 | 147.87 | .887       | .853 | 348.25 | .945  | .939 | 174.68 | .337  | .337 | 162.70 |


The results demonstrate that dPads with the two optimization strategies converges much faster.

On Fly-vs-fly, without iterative graph unfolding, the program derivation graph generated by dPads has 33 nodes shared by 485 operations, 32 edges, and a total of 13824 parameters to train. dPads encounters an out-of-memory (OOM) exception when training this graph. In contrast, with iterative graph unfolding, the deepest graph has only 21 nodes shared by 220 operations, 20 edges, and a total of 5497 parameters to train. dPads converges in less than 360 mins.

On Basketball, without node sharing, the deepest program derivation graph generated by dPads has 94 nodes hosting 302 operations, 93 edges, and a total of 8637 parameters to train. The search procedure of dPads (Sec 3.3) does not terminate in 24 hours because of the many nodes and edges. In contrast, with node sharing, the program derivation graph reduces to only 29 nodes shared by 152 operations, 28 edges, and a total of 4157 parameters. dPads converges in less than 180 mins.

Moreover, training without these two optimizations does not necessarily produce better results even when there is not OOM or timeout. For example, on Basketball, dPads achieves .945 F1 score. dPads without iterative graph unfolding only obtains .848 F1 score. We suspect this is because the program derivation graph without iterative unfolding is more difficult to train as it contains significantly more parameters (6791 vs 4157). In contrast, dPads uses top-N preservation to efficiently prune away low-quality options. We will provide additional details in the final version.


## Q4: Lack of study on many other pruning approaches (e.g., considering the top-N candidates across the entire search tree, rather than per node)?

We indeed have explored other pruning approaches for iterative graph unfolding, including reserving the top-N candidates across the entire search graph rather than per node, which we refer to as top-N programs. We also considered another pruning algorithm which we refer to as First Complete First Unfold (FCFU). It leverages A$^*$ search to manage a queue of program derivation graphs. After training converges on a program derivation graph from the queue, FCFU decomposes the graph into several sub graphs which are pushed back to the queue. Once a graph with each node containing at most 1 DSL function is obtained from the queue as the least cost, we immediately unfold the graph into deeper levels and push it back to the queue. FCFU prioritizes to unfold the best partial program observed so far.

We compared dPads with FCFU and top-N programs (as suggested by the reviewer) over 5 random runs. The results are as follows. Costs of time are set in minutes.  

|            | Cim13 |      |        | Fly-vs-fly |      |        | Bball |      |        | Sk152 |      |        |
|------------|-------|------|--------|------------|------|--------|-------|------|--------|-------|------|--------|
|            | **F1**    | **Acc.** | **Time**   | **F1**         | **Acc.** | **Time**   | **F1**    | **Acc.** | **Time**   | **F1**    | **Acc.** | **Time**  |
| **FCFU** | .456  | .813 | 489.53 | .889       | .853 | 606.65 | -     | -    | >24hrs | .338  | .339 | 319.60 |
| **top-5 programs** | .299  | .516 | 184.16 | .652       | .554 | 153.28 | .848  | .829 | 30.59  | .283  | .277 | 62.04  |
| **dPads**      | .458  | .812 | 147.87 | .887       | .853 | 348.25 | .945  | .939 | 174.68 | .337  | .337 | 162.70 |

It can be seen that dPads outperforms top-5 programs preservation, achieving higher accuracy and F1 scores. This is because top-N programs preservation tends to excessively detach many valid DSL functions from graph nodes (especially when N is small), leading to suboptimal final programs. FCFU achieves comparable accuracy and F1 scores with dPads but runs significantly slower than dPads. It even times-out on Basketball. This is because FCFU only unfolds one best partial program each time, causing the search queue to grow exponentially longer with graph decomposition. In contrast, dPads maintains a *set* of high-quality programs via top-N preservation on nodes and simultaneously expands all of these programs via iterative graph unfolding. We will be happy to provide additional details in the final version if the paper is accepted.

## Q5: No discussion of limitation.

As pointed out by the other reviewers, our paper acknowledges that the main technical limitation is that dPads's performance does not match the RNN baseline. This is mainly due to the limitations in expressivity imposed by the DSL. Firstly, in dPads's DSLs, we only allow for customized feature functions $F_{S,\theta}(x)$ that extract a vector consisting of a predefined subset $S$ of the dimensions of an input sequence $x$. For example, for Crim13, we predefine feature functions such as the XY positions, angles, velocity, acceleration, distance of a pair of mice, and distance difference for every two consecutive frames. We list the details of $F_{S,\theta}(x)$ for each dataset in Table 3, 4, 5, 6 in Appendix C. These feature functions are extremely helpful to ensure that a synthesized program composed of these feature functions is interpretable. However, a program limited to these predefined feature functions may be suboptimal as only a subset of features is used. Instead, an RNN policy can understand the whole context of a sequence using all features available from the input. Secondly, for the sake of interpretation, the DSLs also predefine a limited set of algebraic operations to process the outputs from the feature functions. However, an RNN can use a more expressive activation function to learn about long-term dependencies in data. We have reported the performance limitation of dPads in Table 1.

We also mention in Section 6 that there is potential for attacks on security and fairness on programmatic models derived via dPads.

## Q6: Can the first parameter of ITE and Add share parameters? More generally, are types used at all to direct the search?

The ITE function is an abbreviation for "if $\alpha_1$ $\ge$ 0 then $\alpha_2$ else $\alpha_3$". Therefore $\alpha_1$ does not have a Boolean type and we allow for nested ITEs.

Programs in our DSLs operate over two data types: real vectors and sequences of real vectors. We assume a simple type system that makes sure that these types are used consistently. The type system is quite basic and primarily ensures that the types of formal parameters, actual parameters, and return values are as expected. When expanding a partial architecture, we ensure that the chosen expansion is consistent w.r.t. this type system. We will provide additional details about the type system in the final version.

## Q7: DSL is extremely strange, as it mixes non-standard, apparently task-focused primitives.

The programming language in Figure 1 is a domain-specific language (DSL), specific to the task of sequence classification. Although dPads is generally applicable in many applications, we specifically study it in the sequence classification context. Thus, we sketch our DSL for this domain in the paper. For example, the DSL includes a set of higher-order combinators to recurse over sequences that aim to compactly express sequence-to-sequence functions.

The way that we define our DSLs is standard in research on Programming Languages (PL), and also appears in many prior papers in the intersection of PL and Machine Learning.

## Q8: How are variables picked? Does the grammar contain a fixed pool of variables?

Like many other DSLs for program synthesis, the programming language in Figure 1 is purely functional. Thus it only needs to deal with the input sequence $x$. We also allow for customized feature functions $F_{S,\theta}(x)$ that extract a vector consisting of a predefined subset $S$ of the dimensions of a sequence $x$ and pass the extracted vector through a linear function with trainable parameters $\theta$.

## Q9: Does N=3 mean that all children are always preserved?

N=3 does not mean that we preserve all children. The program derivation graph in Figure 3 is only an example. In our experiment, a node typically contains more than 3 operations as our DSLs contain a lot more than 3 functions. We give the full details of our DSLs in Appendix C.

## Q10: Left half of Table 2 for "Sk152-10 actions" is a copy of the Crim13-sniff results.

We apology for the confusion. We have corrected Table 2 in Appendix A.

**Other Comments.** In addition to these main points, we want to thank the reviewer for the helpful suggestions on how to further improve the paper. We plan on incorporating all of these, such as exactly defining the symbols $w$ and $f$, commenting on the choice of DSL functions, and discussing the limitations of dPads in depth in the final version.
